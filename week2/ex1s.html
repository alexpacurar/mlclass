<!DOCTYPE html><html>
<head>
<title></title>
<style type="text/css">
<!--
.xflip {
    -moz-transform: scaleX(-1);
    -webkit-transform: scaleX(-1);
    -o-transform: scaleX(-1);
    transform: scaleX(-1);
    filter: fliph;
}
.yflip {
    -moz-transform: scaleY(-1);
    -webkit-transform: scaleY(-1);
    -o-transform: scaleY(-1);
    transform: scaleY(-1);
    filter: flipv;
}
.xyflip {
    -moz-transform: scaleX(-1) scaleY(-1);
    -webkit-transform: scaleX(-1) scaleY(-1);
    -o-transform: scaleX(-1) scaleY(-1);
    transform: scaleX(-1) scaleY(-1);
    filter: fliph + flipv;
}
-->
</style>
</head>
<body>
<a name=1></a>Programming&#160;Exercise&#160;1:&#160;Linear&#160;Regression<br/>
Machine&#160;Learning<br/>
Introduction<br/>
In&#160;this&#160;exercise,&#160;you&#160;will&#160;implement&#160;linear&#160;regression&#160;and&#160;get&#160;to&#160;see&#160;it&#160;work<br/>on&#160;data.&#160;Before&#160;starting&#160;on&#160;this&#160;programming&#160;exercise,&#160;we&#160;strongly&#160;recom-<br/>mend&#160;watching&#160;the&#160;video&#160;lectures&#160;and&#160;completing&#160;the&#160;review&#160;questions&#160;for<br/>the&#160;associated&#160;topics.<br/>
To&#160;get&#160;started&#160;with&#160;the&#160;exercise,&#160;you&#160;will&#160;need&#160;to&#160;download&#160;the&#160;starter<br/>
code&#160;and&#160;unzip&#160;its&#160;contents&#160;to&#160;the&#160;directory&#160;where&#160;you&#160;wish&#160;to&#160;complete&#160;the<br/>exercise.&#160;If&#160;needed,&#160;use&#160;the&#160;cd&#160;command&#160;in&#160;Octave&#160;to&#160;change&#160;to&#160;this&#160;directory<br/>before&#160;starting&#160;this&#160;exercise.<br/>
You&#160;can&#160;also&#160;find&#160;instructions&#160;for&#160;installing&#160;Octave&#160;on&#160;the&#160;“Octave&#160;In-<br/>
stallation”&#160;page&#160;on&#160;the&#160;course&#160;website.<br/>
Files&#160;included&#160;in&#160;this&#160;exercise<br/>
ex1.m&#160;-&#160;Octave&#160;script&#160;that&#160;will&#160;help&#160;step&#160;you&#160;through&#160;the&#160;exercise<br/>ex1&#160;multi.m&#160;-&#160;Octave&#160;script&#160;for&#160;the&#160;later&#160;parts&#160;of&#160;the&#160;exercise<br/>ex1data1.txt&#160;-&#160;Dataset&#160;for&#160;linear&#160;regression&#160;with&#160;one&#160;variable<br/>ex1data2.txt&#160;-&#160;Dataset&#160;for&#160;linear&#160;regression&#160;with&#160;multiple&#160;variables<br/>submit.m&#160;-&#160;Submission&#160;script&#160;that&#160;sends&#160;your&#160;solutions&#160;to&#160;our&#160;servers<br/>[&#160;]&#160;warmUpExercise.m&#160;-&#160;Simple&#160;example&#160;function&#160;in&#160;Octave<br/>[&#160;]&#160;plotData.m&#160;-&#160;Function&#160;to&#160;display&#160;the&#160;dataset<br/>[&#160;]&#160;computeCost.m&#160;-&#160;Function&#160;to&#160;compute&#160;the&#160;cost&#160;of&#160;linear&#160;regression<br/>[&#160;]&#160;gradientDescent.m&#160;-&#160;Function&#160;to&#160;run&#160;gradient&#160;descent<br/>[†]&#160;computeCostMulti.m&#160;-&#160;Cost&#160;function&#160;for&#160;multiple&#160;variables<br/>[†]&#160;gradientDescentMulti.m&#160;-&#160;Gradient&#160;descent&#160;for&#160;multiple&#160;variables<br/>[†]&#160;featureNormalize.m&#160;-&#160;Function&#160;to&#160;normalize&#160;features<br/>[†]&#160;normalEqn.m&#160;-&#160;Function&#160;to&#160;compute&#160;the&#160;normal&#160;equations<br/>
indicates&#160;files&#160;you&#160;will&#160;need&#160;to&#160;complete<br/>
†&#160;indicates&#160;extra&#160;credit&#160;exercises<br/>
1<br/>
<hr/>
<a name=2></a>Throughout&#160;the&#160;exercise,&#160;you&#160;will&#160;be&#160;using&#160;the&#160;scripts&#160;ex1.m&#160;and&#160;ex1&#160;multi.m.<br/>
These&#160;scripts&#160;set&#160;up&#160;the&#160;dataset&#160;for&#160;the&#160;problems&#160;and&#160;make&#160;calls&#160;to&#160;functions<br/>that&#160;you&#160;will&#160;write.&#160;You&#160;do&#160;not&#160;need&#160;to&#160;modify&#160;either&#160;of&#160;them.&#160;You&#160;are&#160;only<br/>required&#160;to&#160;modify&#160;functions&#160;in&#160;other&#160;files,&#160;by&#160;following&#160;the&#160;instructions&#160;in<br/>this&#160;assignment.<br/>
For&#160;this&#160;programming&#160;exercise,&#160;you&#160;are&#160;only&#160;required&#160;to&#160;complete&#160;the&#160;first<br/>
part&#160;of&#160;the&#160;exercise&#160;to&#160;implement&#160;linear&#160;regression&#160;with&#160;one&#160;variable.&#160;The<br/>second&#160;part&#160;of&#160;the&#160;exercise,&#160;which&#160;you&#160;may&#160;complete&#160;for&#160;extra&#160;credit,&#160;covers<br/>linear&#160;regression&#160;with&#160;multiple&#160;variables.<br/>
Where&#160;to&#160;get&#160;help<br/>
The&#160;exercises&#160;in&#160;this&#160;course&#160;use&#160;Octav<a href="ex1s.html#2">e,1&#160;</a>a&#160;high-level&#160;programming&#160;language<br/>well-suited&#160;for&#160;numerical&#160;computations.&#160;If&#160;you&#160;do&#160;not&#160;have&#160;Octave&#160;installed,<br/>please&#160;refer&#160;to&#160;the&#160;installation&#160;instructons&#160;at&#160;the&#160;“Octave&#160;Installation”&#160;page<br/>on&#160;the&#160;course&#160;website.<br/>
At&#160;the&#160;Octave&#160;command&#160;line,&#160;typing&#160;help&#160;followed&#160;by&#160;a&#160;function&#160;name<br/>
displays&#160;documentation&#160;for&#160;a&#160;built-in&#160;function.&#160;For&#160;example,&#160;help&#160;plot&#160;will<br/>bring&#160;up&#160;help&#160;information&#160;for&#160;plotting.&#160;Further&#160;documentation&#160;for&#160;Octave<br/>functions&#160;can&#160;be&#160;found&#160;at&#160;the&#160;<a href="http://www.gnu.org/software/octave/doc/interpreter/">Octave&#160;documentation&#160;pages.</a><br/>
We&#160;also&#160;strongly&#160;encourage&#160;using&#160;the&#160;online&#160;Q&amp;A&#160;Forum&#160;to&#160;discuss<br/>
exercises&#160;with&#160;other&#160;students.<br/>
However,&#160;do&#160;not&#160;look&#160;at&#160;any&#160;source&#160;code<br/>
written&#160;by&#160;others&#160;or&#160;share&#160;your&#160;source&#160;code&#160;with&#160;others.<br/>
1<br/>
Simple&#160;octave&#160;function<br/>
The&#160;first&#160;part&#160;of&#160;ex1.m&#160;gives&#160;you&#160;practice&#160;with&#160;Octave&#160;syntax&#160;and&#160;the&#160;home-<br/>work&#160;submission&#160;process.&#160;In&#160;the&#160;file&#160;warmUpExercise.m,&#160;you&#160;will&#160;find&#160;the<br/>outline&#160;of&#160;an&#160;Octave&#160;function.&#160;Modify&#160;it&#160;to&#160;return&#160;a&#160;5&#160;x&#160;5&#160;identity&#160;matrix&#160;by<br/>filling&#160;in&#160;the&#160;following&#160;code:<br/>
A&#160;=&#160;eye(5);<br/>
When&#160;you&#160;are&#160;finished,&#160;run&#160;ex1.m&#160;(assuming&#160;you&#160;are&#160;in&#160;the&#160;correct&#160;direc-<br/>
tory,&#160;type&#160;“ex1”&#160;at&#160;the&#160;Octave&#160;prompt)&#160;and&#160;you&#160;should&#160;see&#160;output&#160;similar<br/>to&#160;the&#160;following:<br/>
1Octave&#160;is&#160;a&#160;free&#160;alternative&#160;to&#160;MATLAB.&#160;For&#160;the&#160;programming&#160;exercises,&#160;you&#160;are&#160;free<br/>
to&#160;use&#160;either&#160;Octave&#160;or&#160;MATLAB.<br/>
2<br/>
<hr/>
<a name=3></a>ans&#160;=<br/>
Diagonal&#160;Matrix<br/>
1<br/>
0<br/>
0<br/>
0<br/>
0<br/>
0<br/>
1<br/>
0<br/>
0<br/>
0<br/>
0<br/>
0<br/>
1<br/>
0<br/>
0<br/>
0<br/>
0<br/>
0<br/>
1<br/>
0<br/>
0<br/>
0<br/>
0<br/>
0<br/>
1<br/>
Now&#160;ex1.m&#160;will&#160;pause&#160;until&#160;you&#160;press&#160;any&#160;key,&#160;and&#160;then&#160;will&#160;run&#160;the&#160;code<br/>
for&#160;the&#160;next&#160;part&#160;of&#160;the&#160;assignment.&#160;If&#160;you&#160;wish&#160;to&#160;quit,&#160;typing&#160;ctrl-c&#160;will<br/>stop&#160;the&#160;program&#160;in&#160;the&#160;middle&#160;of&#160;its&#160;run.<br/>
1.1<br/>
Submitting&#160;Solutions<br/>
After&#160;completing&#160;a&#160;part&#160;of&#160;the&#160;exercise,&#160;you&#160;can&#160;submit&#160;your&#160;solutions&#160;for<br/>grading&#160;by&#160;typing&#160;submit&#160;at&#160;the&#160;Octave&#160;command&#160;line.&#160;The&#160;submission<br/>script&#160;will&#160;prompt&#160;you&#160;for&#160;your&#160;username&#160;and&#160;password&#160;and&#160;ask&#160;you&#160;which<br/>files&#160;you&#160;want&#160;to&#160;submit.&#160;You&#160;can&#160;obtain&#160;a&#160;submission&#160;password&#160;from&#160;the<br/>website’s&#160;“Programming&#160;Exercises”&#160;page.<br/>
You&#160;should&#160;now&#160;submit&#160;the&#160;warm&#160;up&#160;exercise.<br/>
You&#160;are&#160;allowed&#160;to&#160;submit&#160;your&#160;solutions&#160;multiple&#160;times,&#160;and&#160;we&#160;will&#160;take<br/>
only&#160;the&#160;highest&#160;score&#160;into&#160;consideration.&#160;To&#160;prevent&#160;rapid-fire&#160;guessing,&#160;the<br/>system&#160;enforces&#160;a&#160;minimum&#160;of&#160;5&#160;minutes&#160;between&#160;submissions.<br/>
2<br/>
Linear&#160;regression&#160;with&#160;one&#160;variable<br/>
In&#160;this&#160;part&#160;of&#160;this&#160;exercise,&#160;you&#160;will&#160;implement&#160;linear&#160;regression&#160;with&#160;one<br/>variable&#160;to&#160;predict&#160;profits&#160;for&#160;a&#160;food&#160;truck.&#160;Suppose&#160;you&#160;are&#160;the&#160;CEO&#160;of&#160;a<br/>restaurant&#160;franchise&#160;and&#160;are&#160;considering&#160;different&#160;cities&#160;for&#160;opening&#160;a&#160;new<br/>outlet.&#160;The&#160;chain&#160;already&#160;has&#160;trucks&#160;in&#160;various&#160;cities&#160;and&#160;you&#160;have&#160;data&#160;for<br/>profits&#160;and&#160;populations&#160;from&#160;the&#160;cities.<br/>
You&#160;would&#160;like&#160;to&#160;use&#160;this&#160;data&#160;to&#160;help&#160;you&#160;select&#160;which&#160;city&#160;to&#160;expand<br/>
to&#160;next.<br/>
3<br/>
<hr/>
<a name=4></a>The&#160;file&#160;ex1data1.txt&#160;contains&#160;the&#160;dataset&#160;for&#160;our&#160;linear&#160;regression&#160;prob-<br/>
lem.&#160;The&#160;first&#160;column&#160;is&#160;the&#160;population&#160;of&#160;a&#160;city&#160;and&#160;the&#160;second&#160;column&#160;is<br/>the&#160;profit&#160;of&#160;a&#160;food&#160;truck&#160;in&#160;that&#160;city.&#160;A&#160;negative&#160;value&#160;for&#160;profit&#160;indicates&#160;a<br/>loss.<br/>
The&#160;ex1.m&#160;script&#160;has&#160;already&#160;been&#160;set&#160;up&#160;to&#160;load&#160;this&#160;data&#160;for&#160;you.<br/>
2.1<br/>
Plotting&#160;the&#160;Data<br/>
Before&#160;starting&#160;on&#160;any&#160;task,&#160;it&#160;is&#160;often&#160;useful&#160;to&#160;understand&#160;the&#160;data&#160;by<br/>visualizing&#160;it.&#160;For&#160;this&#160;dataset,&#160;you&#160;can&#160;use&#160;a&#160;scatter&#160;plot&#160;to&#160;visualize&#160;the<br/>data,&#160;since&#160;it&#160;has&#160;only&#160;two&#160;properties&#160;to&#160;plot&#160;(profit&#160;and&#160;population).&#160;(Many<br/>other&#160;problems&#160;that&#160;you&#160;will&#160;encounter&#160;in&#160;real&#160;life&#160;are&#160;multi-dimensional&#160;and<br/>can’t&#160;be&#160;plotted&#160;on&#160;a&#160;2-d&#160;plot.)<br/>
In&#160;ex1.m,&#160;the&#160;dataset&#160;is&#160;loaded&#160;from&#160;the&#160;data&#160;file&#160;into&#160;the&#160;variables&#160;X<br/>
and&#160;y:<br/>
data&#160;=&#160;load('ex1data1.txt');<br/>
%&#160;read&#160;comma&#160;separated&#160;data<br/>
X&#160;=&#160;data(:,&#160;1);&#160;y&#160;=&#160;data(:,&#160;2);<br/>
m&#160;=&#160;length(y);<br/>
%&#160;number&#160;of&#160;training&#160;examples<br/>
Next,&#160;the&#160;script&#160;calls&#160;the&#160;plotData&#160;function&#160;to&#160;create&#160;a&#160;scatter&#160;plot&#160;of<br/>
the&#160;data.&#160;Your&#160;job&#160;is&#160;to&#160;complete&#160;plotData.m&#160;to&#160;draw&#160;the&#160;plot;&#160;modify&#160;the<br/>file&#160;and&#160;fill&#160;in&#160;the&#160;following&#160;code:<br/>
plot(x,&#160;y,&#160;'rx',&#160;'MarkerSize',&#160;10);<br/>
%&#160;Plot&#160;the&#160;data<br/>
ylabel('Profit&#160;in&#160;$10,000s');<br/>
%&#160;Set&#160;the&#160;y−axis&#160;label<br/>
xlabel('Population&#160;of&#160;City&#160;in&#160;10,000s');<br/>
%&#160;Set&#160;the&#160;x−axis&#160;label<br/>
Now,&#160;when&#160;you&#160;continue&#160;to&#160;run&#160;ex1.m,&#160;our&#160;end&#160;result&#160;should&#160;look&#160;like<br/>
Figure&#160;<a href="ex1s.html#5">1,&#160;</a>with&#160;the&#160;same&#160;red&#160;“x”&#160;markers&#160;and&#160;axis&#160;labels.<br/>
To&#160;learn&#160;more&#160;about&#160;the&#160;plot&#160;command,&#160;you&#160;can&#160;type&#160;help&#160;plot&#160;at&#160;the<br/>
Octave&#160;command&#160;prompt&#160;or&#160;to&#160;search&#160;online&#160;for&#160;plotting&#160;documentation.&#160;(To<br/>change&#160;the&#160;markers&#160;to&#160;red&#160;“x”,&#160;we&#160;used&#160;the&#160;option&#160;‘rx’&#160;together&#160;with&#160;the&#160;plot<br/>command,&#160;i.e.,&#160;plot(..,[your&#160;options&#160;here],..,&#160;‘rx’);&#160;)<br/>
2.2<br/>
Gradient&#160;Descent<br/>
In&#160;this&#160;part,&#160;you&#160;will&#160;fit&#160;the&#160;linear&#160;regression&#160;parameters&#160;θ&#160;to&#160;our&#160;dataset<br/>using&#160;gradient&#160;descent.<br/>
4<br/>
<hr/>
<a name=5></a>25<br/>
20<br/>
15<br/>
10<br/>
Profit in $10,000s<br/>
5<br/>
0<br/>
−54<br/>
6<br/>
8<br/>
10<br/>
12<br/>
14<br/>
16<br/>
18<br/>
20<br/>
22<br/>
24<br/>
Population of City in 10,000s<br/>
Figure&#160;1:&#160;Scatter&#160;plot&#160;of&#160;training&#160;data<br/>
2.2.1<br/>
Update&#160;Equations<br/>
The&#160;objective&#160;of&#160;linear&#160;regression&#160;is&#160;to&#160;minimize&#160;the&#160;cost&#160;function<br/>
m<br/>
1<br/>
J&#160;(θ)&#160;=<br/>
hθ(x(i))&#160;−&#160;y(i)&#160;2<br/>
2m&#160;i=1<br/>
where&#160;the&#160;hypothesis&#160;hθ(x)&#160;is&#160;given&#160;by&#160;the&#160;linear&#160;model<br/>
hθ(x)&#160;=&#160;θT&#160;x&#160;=&#160;θ0&#160;+&#160;θ1x1<br/>
Recall&#160;that&#160;the&#160;parameters&#160;of&#160;your&#160;model&#160;are&#160;the&#160;θj&#160;values.&#160;These&#160;are<br/>
the&#160;values&#160;you&#160;will&#160;adjust&#160;to&#160;minimize&#160;cost&#160;J&#160;(θ).&#160;One&#160;way&#160;to&#160;do&#160;this&#160;is&#160;to<br/>use&#160;the&#160;batch&#160;gradient&#160;descent&#160;algorithm.&#160;In&#160;batch&#160;gradient&#160;descent,&#160;each<br/>iteration&#160;performs&#160;the&#160;update<br/>
m<br/>
1<br/>
θj&#160;:=&#160;θj&#160;−&#160;α<br/>
(hθ(x(i))&#160;−&#160;y(i))x(i)<br/>
(simultaneously&#160;update&#160;θj&#160;for&#160;all&#160;j).<br/>
m<br/>
j<br/>
i=1<br/>
With&#160;each&#160;step&#160;of&#160;gradient&#160;descent,&#160;your&#160;parameters&#160;θj&#160;come&#160;closer&#160;to&#160;the<br/>
5<br/>
<hr/>
<a name=6></a>optimal&#160;values&#160;that&#160;will&#160;achieve&#160;the&#160;lowest&#160;cost&#160;J&#160;(θ).<br/>
Implementation&#160;Note:&#160;We&#160;store&#160;each&#160;example&#160;as&#160;a&#160;row&#160;in&#160;the&#160;the&#160;X<br/>matrix&#160;in&#160;Octave.&#160;To&#160;take&#160;into&#160;account&#160;the&#160;intercept&#160;term&#160;(θ0),&#160;we&#160;add<br/>an&#160;additional&#160;first&#160;column&#160;to&#160;X&#160;and&#160;set&#160;it&#160;to&#160;all&#160;ones.&#160;This&#160;allows&#160;us&#160;to<br/>treat&#160;θ0&#160;as&#160;simply&#160;another&#160;‘feature’.<br/>
2.2.2<br/>
Implementation<br/>
In&#160;ex1.m,&#160;we&#160;have&#160;already&#160;set&#160;up&#160;the&#160;data&#160;for&#160;linear&#160;regression.&#160;In&#160;the<br/>following&#160;lines,&#160;we&#160;add&#160;another&#160;dimension&#160;to&#160;our&#160;data&#160;to&#160;accommodate&#160;the<br/>θ0&#160;intercept&#160;term.&#160;We&#160;also&#160;initialize&#160;the&#160;initial&#160;parameters&#160;to&#160;0&#160;and&#160;the<br/>learning&#160;rate&#160;alpha&#160;to&#160;0.01.<br/>
X&#160;=&#160;[ones(m,&#160;1),&#160;data(:,1)];&#160;%&#160;Add&#160;a&#160;column&#160;of&#160;ones&#160;to&#160;x<br/>
theta&#160;=&#160;zeros(2,&#160;1);&#160;%&#160;initialize&#160;fitting&#160;parameters<br/>
iterations&#160;=&#160;1500;<br/>
alpha&#160;=&#160;0.01;<br/>
2.2.3<br/>
Computing&#160;the&#160;cost&#160;J&#160;(θ)<br/>
As&#160;you&#160;perform&#160;gradient&#160;descent&#160;to&#160;learn&#160;minimize&#160;the&#160;cost&#160;function&#160;J&#160;(θ),<br/>it&#160;is&#160;helpful&#160;to&#160;monitor&#160;the&#160;convergence&#160;by&#160;computing&#160;the&#160;cost.<br/>
In&#160;this<br/>
section,&#160;you&#160;will&#160;implement&#160;a&#160;function&#160;to&#160;calculate&#160;J&#160;(θ)&#160;so&#160;you&#160;can&#160;check&#160;the<br/>convergence&#160;of&#160;your&#160;gradient&#160;descent&#160;implementation.<br/>
Your&#160;next&#160;task&#160;is&#160;to&#160;complete&#160;the&#160;code&#160;in&#160;the&#160;file&#160;computeCost.m,&#160;which<br/>
is&#160;a&#160;function&#160;that&#160;computes&#160;J&#160;(θ).&#160;As&#160;you&#160;are&#160;doing&#160;this,&#160;remember&#160;that&#160;the<br/>variables&#160;X&#160;and&#160;y&#160;are&#160;not&#160;scalar&#160;values,&#160;but&#160;matrices&#160;whose&#160;rows&#160;represent<br/>the&#160;examples&#160;from&#160;the&#160;training&#160;set.<br/>
Once&#160;you&#160;have&#160;completed&#160;the&#160;function,&#160;the&#160;next&#160;step&#160;in&#160;ex1.m&#160;will&#160;run<br/>
computeCost&#160;once&#160;using&#160;θ&#160;initialized&#160;to&#160;zeros,&#160;and&#160;you&#160;will&#160;see&#160;the&#160;cost<br/>printed&#160;to&#160;the&#160;screen.<br/>
You&#160;should&#160;expect&#160;to&#160;see&#160;a&#160;cost&#160;of&#160;32.07.<br/>
You&#160;should&#160;now&#160;submit&#160;“compute&#160;cost”&#160;for&#160;linear&#160;regression&#160;with&#160;one<br/>
variable.<br/>
6<br/>
<hr/>
<a name=7></a>2.2.4<br/>
Gradient&#160;descent<br/>
Next,&#160;you&#160;will&#160;implement&#160;gradient&#160;descent&#160;in&#160;the&#160;file&#160;gradientDescent.m.<br/>The&#160;loop&#160;structure&#160;has&#160;been&#160;written&#160;for&#160;you,&#160;and&#160;you&#160;only&#160;need&#160;to&#160;supply<br/>the&#160;updates&#160;to&#160;θ&#160;within&#160;each&#160;iteration.<br/>
As&#160;you&#160;program,&#160;make&#160;sure&#160;you&#160;understand&#160;what&#160;you&#160;are&#160;trying&#160;to&#160;opti-<br/>
mize&#160;and&#160;what&#160;is&#160;being&#160;updated.&#160;Keep&#160;in&#160;mind&#160;that&#160;the&#160;cost&#160;J&#160;(θ)&#160;is&#160;parame-<br/>terized&#160;by&#160;the&#160;vector&#160;θ,&#160;not&#160;X&#160;and&#160;y.&#160;That&#160;is,&#160;we&#160;minimize&#160;the&#160;value&#160;of&#160;J&#160;(θ)<br/>by&#160;changing&#160;the&#160;values&#160;of&#160;the&#160;vector&#160;θ,&#160;not&#160;by&#160;changing&#160;X&#160;or&#160;y.&#160;Refer&#160;to&#160;the<br/>equations&#160;in&#160;this&#160;handout&#160;and&#160;to&#160;the&#160;video&#160;lectures&#160;if&#160;you&#160;are&#160;uncertain.<br/>
A&#160;good&#160;way&#160;to&#160;verify&#160;that&#160;gradient&#160;descent&#160;is&#160;working&#160;correctly&#160;is&#160;to&#160;look<br/>
at&#160;the&#160;value&#160;of&#160;J&#160;(θ)&#160;and&#160;check&#160;that&#160;it&#160;is&#160;decreasing&#160;with&#160;each&#160;step.&#160;The<br/>starter&#160;code&#160;for&#160;gradientDescent.m&#160;calls&#160;computeCost&#160;on&#160;every&#160;iteration<br/>and&#160;prints&#160;the&#160;cost.&#160;Assuming&#160;you&#160;have&#160;implemented&#160;gradient&#160;descent&#160;and<br/>computeCost&#160;correctly,&#160;your&#160;value&#160;of&#160;J&#160;(θ)&#160;should&#160;never&#160;increase,&#160;and&#160;should<br/>converge&#160;to&#160;a&#160;steady&#160;value&#160;by&#160;the&#160;end&#160;of&#160;the&#160;algorithm.<br/>
After&#160;you&#160;are&#160;finished,&#160;ex1.m&#160;will&#160;use&#160;your&#160;final&#160;parameters&#160;to&#160;plot&#160;the<br/>
linear&#160;fit.&#160;The&#160;result&#160;should&#160;look&#160;something&#160;like&#160;Figure&#160;<a href="ex1s.html#8">2:</a><br/>
Your&#160;final&#160;values&#160;for&#160;θ&#160;will&#160;also&#160;be&#160;used&#160;to&#160;make&#160;predictions&#160;on&#160;profits&#160;in<br/>
areas&#160;of&#160;35,000&#160;and&#160;70,000&#160;people.&#160;Note&#160;the&#160;way&#160;that&#160;the&#160;following&#160;lines&#160;in<br/>ex1.m&#160;uses&#160;matrix&#160;multiplication,&#160;rather&#160;than&#160;explicit&#160;summation&#160;or&#160;loop-<br/>ing,&#160;to&#160;calculate&#160;the&#160;predictions.&#160;This&#160;is&#160;an&#160;example&#160;of&#160;code&#160;vectorization&#160;in<br/>Octave.<br/>
You&#160;should&#160;now&#160;submit&#160;gradient&#160;descent&#160;for&#160;linear&#160;regression&#160;with&#160;one<br/>
variable.<br/>
predict1&#160;=&#160;[1,&#160;3.5]&#160;*&#160;theta;<br/>predict2&#160;=&#160;[1,&#160;7]&#160;*&#160;theta;<br/>
2.3<br/>
Debugging<br/>
Here&#160;are&#160;some&#160;things&#160;to&#160;keep&#160;in&#160;mind&#160;as&#160;you&#160;implement&#160;gradient&#160;descent:<br/>
❼&#160;Octave&#160;array&#160;indices&#160;start&#160;from&#160;one,&#160;not&#160;zero.&#160;If&#160;you’re&#160;storing&#160;θ0&#160;and<br/>
θ1&#160;in&#160;a&#160;vector&#160;called&#160;theta,&#160;the&#160;values&#160;will&#160;be&#160;theta(1)&#160;and&#160;theta(2).<br/>
❼&#160;If&#160;you&#160;are&#160;seeing&#160;many&#160;errors&#160;at&#160;runtime,&#160;inspect&#160;your&#160;matrix&#160;operations<br/>
to&#160;make&#160;sure&#160;that&#160;you’re&#160;adding&#160;and&#160;multiplying&#160;matrices&#160;of&#160;compat-<br/>ible&#160;dimensions.&#160;Printing&#160;the&#160;dimensions&#160;of&#160;variables&#160;with&#160;the&#160;size<br/>command&#160;will&#160;help&#160;you&#160;debug.<br/>
7<br/>
<hr/>
<a name=8></a>25<br/>
&#160;<br/>
20<br/>
15<br/>
10<br/>
Profit in $10,000s<br/>
5<br/>
Training data<br/>
0<br/>
Linear regression<br/>
−5&#160;&#160;4<br/>
6<br/>
8<br/>
10<br/>
12<br/>
14<br/>
16<br/>
18<br/>
20<br/>
22<br/>
24<br/>
Population of City in 10,000s<br/>
Figure&#160;2:&#160;Training&#160;data&#160;with&#160;linear&#160;regression&#160;fit<br/>
❼&#160;By&#160;default,&#160;Octave&#160;interprets&#160;math&#160;operators&#160;to&#160;be&#160;matrix&#160;operators.<br/>
This&#160;is&#160;a&#160;common&#160;source&#160;of&#160;size&#160;incompatibility&#160;errors.&#160;If&#160;you&#160;don’t&#160;want<br/>matrix&#160;multiplication,&#160;you&#160;need&#160;to&#160;add&#160;the&#160;“dot”&#160;notation&#160;to&#160;specify&#160;this<br/>to&#160;Octave.&#160;For&#160;example,&#160;A*B&#160;does&#160;a&#160;matrix&#160;multiply,&#160;while&#160;A.*B&#160;does<br/>an&#160;element-wise&#160;multiplication.<br/>
2.4<br/>
Visualizing&#160;J&#160;(θ)<br/>
To&#160;understand&#160;the&#160;cost&#160;function&#160;J&#160;(θ)&#160;better,&#160;you&#160;will&#160;now&#160;plot&#160;the&#160;cost&#160;over<br/>a&#160;2-dimensional&#160;grid&#160;of&#160;θ0&#160;and&#160;θ1&#160;values.&#160;You&#160;will&#160;not&#160;need&#160;to&#160;code&#160;anything<br/>new&#160;for&#160;this&#160;part,&#160;but&#160;you&#160;should&#160;understand&#160;how&#160;the&#160;code&#160;you&#160;have&#160;written<br/>already&#160;is&#160;creating&#160;these&#160;images.<br/>
In&#160;the&#160;next&#160;step&#160;of&#160;ex1.m,&#160;there&#160;is&#160;code&#160;set&#160;up&#160;to&#160;calculate&#160;J&#160;(θ)&#160;over&#160;a<br/>
grid&#160;of&#160;values&#160;using&#160;the&#160;computeCost&#160;function&#160;that&#160;you&#160;wrote.<br/>
8<br/>
<hr/>
<a name=9></a>%&#160;initialize&#160;J&#160;vals&#160;to&#160;a&#160;matrix&#160;of&#160;0's<br/>
J&#160;vals&#160;=&#160;zeros(length(theta0&#160;vals),&#160;length(theta1&#160;vals));<br/>
%&#160;Fill&#160;out&#160;J&#160;vals<br/>
for&#160;i&#160;=&#160;1:length(theta0&#160;vals)<br/>
for&#160;j&#160;=&#160;1:length(theta1&#160;vals)<br/>
t&#160;=&#160;[theta0&#160;vals(i);&#160;theta1&#160;vals(j)];<br/>
J&#160;vals(i,j)&#160;=&#160;computeCost(x,&#160;y,&#160;t);<br/>
end<br/>
end<br/>
After&#160;these&#160;lines&#160;are&#160;executed,&#160;you&#160;will&#160;have&#160;a&#160;2-D&#160;array&#160;of&#160;J&#160;(θ)&#160;values.<br/>
The&#160;script&#160;ex1.m&#160;will&#160;then&#160;use&#160;these&#160;values&#160;to&#160;produce&#160;surface&#160;and&#160;contour<br/>plots&#160;of&#160;J&#160;(θ)&#160;using&#160;the&#160;surf&#160;and&#160;contour&#160;commands.&#160;The&#160;plots&#160;should&#160;look<br/>something&#160;like&#160;Figure&#160;<a href="ex1s.html#9">3:</a><br/>
4<br/>
3.5<br/>
800<br/>
3<br/>
700<br/>
600<br/>
2.5<br/>
500<br/>
2<br/>
400<br/>
θ&#160;1&#160;1.5<br/>
300<br/>
200<br/>
1<br/>
100<br/>
0.5<br/>
0<br/>
4<br/>
0<br/>
3<br/>
10<br/>
2<br/>
5<br/>
−0.5<br/>
1<br/>
0<br/>
0<br/>
−5<br/>
−1<br/>
−10<br/>
−8<br/>
−6<br/>
−4<br/>
−2<br/>
0<br/>
2<br/>
4<br/>
6<br/>
8<br/>
10<br/>
−1<br/>
−10<br/>
θ<br/>
θ<br/>
1<br/>
θ<br/>
0<br/>
0<br/>
(a)&#160;Surface<br/>
(b)&#160;Contour,&#160;showing&#160;minimum<br/>
Figure&#160;3:&#160;Cost&#160;function&#160;J&#160;(θ)<br/>
The&#160;purpose&#160;of&#160;these&#160;graphs&#160;is&#160;to&#160;show&#160;you&#160;that&#160;how&#160;J&#160;(θ)&#160;varies&#160;with<br/>
changes&#160;in&#160;θ0&#160;and&#160;θ1.&#160;The&#160;cost&#160;function&#160;J(θ)&#160;is&#160;bowl-shaped&#160;and&#160;has&#160;a&#160;global<br/>mininum.&#160;(This&#160;is&#160;easier&#160;to&#160;see&#160;in&#160;the&#160;contour&#160;plot&#160;than&#160;in&#160;the&#160;3D&#160;surface<br/>plot).&#160;This&#160;minimum&#160;is&#160;the&#160;optimal&#160;point&#160;for&#160;θ0&#160;and&#160;θ1,&#160;and&#160;each&#160;step&#160;of<br/>gradient&#160;descent&#160;moves&#160;closer&#160;to&#160;this&#160;point.<br/>
9<br/>
<hr/>
<a name=10></a>Extra&#160;Credit&#160;Exercises&#160;(optional)<br/>
If&#160;you&#160;have&#160;successfully&#160;completed&#160;the&#160;material&#160;above,&#160;congratulations!&#160;You<br/>now&#160;understand&#160;linear&#160;regression&#160;and&#160;should&#160;able&#160;to&#160;start&#160;using&#160;it&#160;on&#160;your<br/>own&#160;datasets.<br/>
For&#160;the&#160;rest&#160;of&#160;this&#160;programming&#160;exercise,&#160;we&#160;have&#160;included&#160;the&#160;following<br/>
optional&#160;extra&#160;credit&#160;exercises.&#160;These&#160;exercises&#160;will&#160;help&#160;you&#160;gain&#160;a&#160;deeper<br/>understanding&#160;of&#160;the&#160;material,&#160;and&#160;if&#160;you&#160;are&#160;able&#160;to&#160;do&#160;so,&#160;we&#160;encourage<br/>you&#160;to&#160;complete&#160;them&#160;as&#160;well.<br/>
3<br/>
Linear&#160;regression&#160;with&#160;multiple&#160;variables<br/>
In&#160;this&#160;part,&#160;you&#160;will&#160;implement&#160;linear&#160;regression&#160;with&#160;multiple&#160;variables&#160;to<br/>predict&#160;the&#160;prices&#160;of&#160;houses.&#160;Suppose&#160;you&#160;are&#160;selling&#160;your&#160;house&#160;and&#160;you<br/>want&#160;to&#160;know&#160;what&#160;a&#160;good&#160;market&#160;price&#160;would&#160;be.&#160;One&#160;way&#160;to&#160;do&#160;this&#160;is&#160;to<br/>first&#160;collect&#160;information&#160;on&#160;recent&#160;houses&#160;sold&#160;and&#160;make&#160;a&#160;model&#160;of&#160;housing<br/>prices.<br/>
The&#160;file&#160;ex1data2.txt&#160;contains&#160;a&#160;training&#160;set&#160;of&#160;housing&#160;prices&#160;in&#160;Port-<br/>
land,&#160;Oregon.&#160;The&#160;first&#160;column&#160;is&#160;the&#160;size&#160;of&#160;the&#160;house&#160;(in&#160;square&#160;feet),&#160;the<br/>second&#160;column&#160;is&#160;the&#160;number&#160;of&#160;bedrooms,&#160;and&#160;the&#160;third&#160;column&#160;is&#160;the&#160;price<br/>of&#160;the&#160;house.<br/>
The&#160;ex1&#160;multi.m&#160;script&#160;has&#160;been&#160;set&#160;up&#160;to&#160;help&#160;you&#160;step&#160;through&#160;this<br/>
exercise.<br/>
3.1<br/>
Feature&#160;Normalization<br/>
The&#160;ex1&#160;multi.m&#160;script&#160;will&#160;start&#160;by&#160;loading&#160;and&#160;displaying&#160;some&#160;values<br/>from&#160;this&#160;dataset.&#160;By&#160;looking&#160;at&#160;the&#160;values,&#160;note&#160;that&#160;house&#160;sizes&#160;are&#160;about<br/>1000&#160;times&#160;the&#160;number&#160;of&#160;bedrooms.&#160;When&#160;features&#160;differ&#160;by&#160;orders&#160;of&#160;mag-<br/>nitude,&#160;first&#160;performing&#160;feature&#160;scaling&#160;can&#160;make&#160;gradient&#160;descent&#160;converge<br/>much&#160;more&#160;quickly.<br/>
Your&#160;task&#160;here&#160;is&#160;to&#160;complete&#160;the&#160;code&#160;in&#160;featureNormalize.m&#160;to<br/>
❼&#160;Subtract&#160;the&#160;mean&#160;value&#160;of&#160;each&#160;feature&#160;from&#160;the&#160;dataset.<br/>
❼&#160;After&#160;subtracting&#160;the&#160;mean,&#160;additionally&#160;scale&#160;(divide)&#160;the&#160;feature&#160;values<br/>
by&#160;their&#160;respective&#160;“standard&#160;deviations.”<br/>
10<br/>
<hr/>
<a name=11></a>The&#160;standard&#160;deviation&#160;is&#160;a&#160;way&#160;of&#160;measuring&#160;how&#160;much&#160;variation&#160;there<br/>
is&#160;in&#160;the&#160;range&#160;of&#160;values&#160;of&#160;a&#160;particular&#160;feature&#160;(most&#160;data&#160;points&#160;will&#160;lie<br/>within&#160;±2&#160;standard&#160;deviations&#160;of&#160;the&#160;mean);&#160;this&#160;is&#160;an&#160;alternative&#160;to&#160;taking<br/>the&#160;range&#160;of&#160;values&#160;(max-min).&#160;In&#160;Octave,&#160;you&#160;can&#160;use&#160;the&#160;“std”&#160;function&#160;to<br/>compute&#160;the&#160;standard&#160;deviation.&#160;For&#160;example,&#160;inside&#160;featureNormalize.m,<br/>the&#160;quantity&#160;X(:,1)&#160;contains&#160;all&#160;the&#160;values&#160;of&#160;x1&#160;(house&#160;sizes)&#160;in&#160;the&#160;training<br/>set,&#160;so&#160;std(X(:,1))&#160;computes&#160;the&#160;standard&#160;deviation&#160;of&#160;the&#160;house&#160;sizes.<br/>At&#160;the&#160;time&#160;that&#160;featureNormalize.m&#160;is&#160;called,&#160;the&#160;extra&#160;column&#160;of&#160;1’s<br/>corresponding&#160;to&#160;x0&#160;=&#160;1&#160;has&#160;not&#160;yet&#160;been&#160;added&#160;to&#160;X&#160;(see&#160;ex1&#160;multi.m&#160;for<br/>details).<br/>
You&#160;will&#160;do&#160;this&#160;for&#160;all&#160;the&#160;features&#160;and&#160;your&#160;code&#160;should&#160;work&#160;with<br/>
datasets&#160;of&#160;all&#160;sizes&#160;(any&#160;number&#160;of&#160;features&#160;/&#160;examples).&#160;Note&#160;that&#160;each<br/>column&#160;of&#160;the&#160;matrix&#160;X&#160;corresponds&#160;to&#160;one&#160;feature.<br/>
You&#160;should&#160;now&#160;submit&#160;feature&#160;normalization.<br/>
Implementation&#160;Note:&#160;When&#160;normalizing&#160;the&#160;features,&#160;it&#160;is&#160;important<br/>to&#160;store&#160;the&#160;values&#160;used&#160;for&#160;normalization&#160;-&#160;the&#160;mean&#160;value&#160;and&#160;the&#160;stan-<br/>dard&#160;deviation&#160;used&#160;for&#160;the&#160;computations.&#160;After&#160;learning&#160;the&#160;parameters<br/>from&#160;the&#160;model,&#160;we&#160;often&#160;want&#160;to&#160;predict&#160;the&#160;prices&#160;of&#160;houses&#160;we&#160;have&#160;not<br/>seen&#160;before.&#160;Given&#160;a&#160;new&#160;x&#160;value&#160;(living&#160;room&#160;area&#160;and&#160;number&#160;of&#160;bed-<br/>rooms),&#160;we&#160;must&#160;first&#160;normalize&#160;x&#160;using&#160;the&#160;mean&#160;and&#160;standard&#160;deviation<br/>that&#160;we&#160;had&#160;previously&#160;computed&#160;from&#160;the&#160;training&#160;set.<br/>
3.2<br/>
Gradient&#160;Descent<br/>
Previously,&#160;you&#160;implemented&#160;gradient&#160;descent&#160;on&#160;a&#160;univariate&#160;regression<br/>problem.&#160;The&#160;only&#160;difference&#160;now&#160;is&#160;that&#160;there&#160;is&#160;one&#160;more&#160;feature&#160;in&#160;the<br/>matrix&#160;X.&#160;The&#160;hypothesis&#160;function&#160;and&#160;the&#160;batch&#160;gradient&#160;descent&#160;update<br/>rule&#160;remain&#160;unchanged.<br/>
You&#160;should&#160;complete&#160;the&#160;code&#160;in&#160;computeCostMulti.m&#160;and&#160;gradientDescentMulti.m<br/>
to&#160;implement&#160;the&#160;cost&#160;function&#160;and&#160;gradient&#160;descent&#160;for&#160;linear&#160;regression&#160;with<br/>multiple&#160;variables.&#160;If&#160;your&#160;code&#160;in&#160;the&#160;previous&#160;part&#160;(single&#160;variable)&#160;already<br/>supports&#160;multiple&#160;variables,&#160;you&#160;can&#160;use&#160;it&#160;here&#160;too.<br/>
Make&#160;sure&#160;your&#160;code&#160;supports&#160;any&#160;number&#160;of&#160;features&#160;and&#160;is&#160;well-vectorized.<br/>
You&#160;can&#160;use&#160;‘size(X,&#160;2)’&#160;to&#160;find&#160;out&#160;how&#160;many&#160;features&#160;are&#160;present&#160;in&#160;the<br/>dataset.<br/>
You&#160;should&#160;now&#160;submit&#160;compute&#160;cost&#160;and&#160;gradient&#160;descent&#160;for&#160;linear&#160;re-<br/>
gression&#160;with&#160;multiple&#160;variables.<br/>
11<br/>
<hr/>
<a name=12></a>Implementation&#160;Note:&#160;In&#160;the&#160;multivariate&#160;case,&#160;the&#160;cost&#160;function&#160;can<br/>also&#160;be&#160;written&#160;in&#160;the&#160;following&#160;vectorized&#160;form:<br/>
1<br/>
J&#160;(θ)&#160;=<br/>
(Xθ&#160;−&#160;y)T&#160;(Xθ&#160;−&#160;y)<br/>
2m<br/>
where<br/>
<br/>
—&#160;(x(1))T&#160;—&#160;<br/>
<br/>
y(1)&#160;<br/>
<br/>
—&#160;(x(2))T&#160;—&#160;<br/>
<br/>
y(2)&#160;<br/>
X&#160;=&#160;<br/>
.<br/>
<br/>
y&#160;=&#160;<br/>
.<br/>
&#160;.<br/>
<br/>
..<br/>
<br/>
<br/>
..<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
—&#160;(x(m))T&#160;—<br/>
y(m)<br/>
The&#160;vectorized&#160;version&#160;is&#160;efficient&#160;when&#160;you’re&#160;working&#160;with&#160;numerical<br/>computing&#160;tools&#160;like&#160;Octave.&#160;If&#160;you&#160;are&#160;an&#160;expert&#160;with&#160;matrix&#160;operations,<br/>you&#160;can&#160;prove&#160;to&#160;yourself&#160;that&#160;the&#160;two&#160;forms&#160;are&#160;equivalent.<br/>
3.2.1<br/>
Optional&#160;(ungraded)&#160;exercise:&#160;Selecting&#160;learning&#160;rates<br/>
In&#160;this&#160;part&#160;of&#160;the&#160;exercise,&#160;you&#160;will&#160;get&#160;to&#160;try&#160;out&#160;different&#160;learning&#160;rates&#160;for<br/>the&#160;dataset&#160;and&#160;find&#160;a&#160;learning&#160;rate&#160;that&#160;converges&#160;quickly.&#160;You&#160;can&#160;change<br/>the&#160;learning&#160;rate&#160;by&#160;modifying&#160;ex1&#160;multi.m&#160;and&#160;changing&#160;the&#160;part&#160;of&#160;the<br/>code&#160;that&#160;sets&#160;the&#160;learning&#160;rate.<br/>
The&#160;next&#160;phase&#160;in&#160;ex1&#160;multi.m&#160;will&#160;call&#160;your&#160;gradientDescent.m&#160;func-<br/>
tion&#160;and&#160;run&#160;gradient&#160;descent&#160;for&#160;about&#160;50&#160;iterations&#160;at&#160;the&#160;chosen&#160;learning<br/>rate.&#160;The&#160;function&#160;should&#160;also&#160;return&#160;the&#160;history&#160;of&#160;J&#160;(θ)&#160;values&#160;in&#160;a&#160;vector<br/>J.&#160;After&#160;the&#160;last&#160;iteration,&#160;the&#160;ex1&#160;multi.m&#160;script&#160;plots&#160;the&#160;J&#160;values&#160;against<br/>the&#160;number&#160;of&#160;the&#160;iterations.<br/>
If&#160;you&#160;picked&#160;a&#160;learning&#160;rate&#160;within&#160;a&#160;good&#160;range,&#160;your&#160;plot&#160;look&#160;similar<br/>
Figure&#160;<a href="ex1s.html#13">4.&#160;</a>If&#160;your&#160;graph&#160;looks&#160;very&#160;different,&#160;especially&#160;if&#160;your&#160;value&#160;of&#160;J&#160;(θ)<br/>increases&#160;or&#160;even&#160;blows&#160;up,&#160;adjust&#160;your&#160;learning&#160;rate&#160;and&#160;try&#160;again.&#160;We&#160;rec-<br/>ommend&#160;trying&#160;values&#160;of&#160;the&#160;learning&#160;rate&#160;α&#160;on&#160;a&#160;log-scale,&#160;at&#160;multiplicative<br/>steps&#160;of&#160;about&#160;3&#160;times&#160;the&#160;previous&#160;value&#160;(i.e.,&#160;0.3,&#160;0.1,&#160;0.03,&#160;0.01&#160;and&#160;so&#160;on).<br/>You&#160;may&#160;also&#160;want&#160;to&#160;adjust&#160;the&#160;number&#160;of&#160;iterations&#160;you&#160;are&#160;running&#160;if&#160;that<br/>will&#160;help&#160;you&#160;see&#160;the&#160;overall&#160;trend&#160;in&#160;the&#160;curve.<br/>
12<br/>
<hr/>
<a name=13></a><img src="ex1-13_1.png"/><br/>
Figure&#160;4:&#160;Convergence&#160;of&#160;gradient&#160;descent&#160;with&#160;an&#160;appropriate&#160;learning&#160;rate<br/>
Implementation&#160;Note:&#160;If&#160;your&#160;learning&#160;rate&#160;is&#160;too&#160;large,&#160;J&#160;(θ)&#160;can&#160;di-<br/>verge&#160;and&#160;‘blow&#160;up’,&#160;resulting&#160;in&#160;values&#160;which&#160;are&#160;too&#160;large&#160;for&#160;computer<br/>calculations.&#160;In&#160;these&#160;situations,&#160;Octave&#160;will&#160;tend&#160;to&#160;return&#160;NaNs.&#160;NaN<br/>stands&#160;for&#160;‘not&#160;a&#160;number’&#160;and&#160;is&#160;often&#160;caused&#160;by&#160;undefined&#160;operations<br/>that&#160;involve&#160;−∞&#160;and&#160;+∞.<br/>
Octave&#160;Tip:&#160;To&#160;compare&#160;how&#160;different&#160;learning&#160;learning&#160;rates&#160;affect<br/>convergence,&#160;it’s&#160;helpful&#160;to&#160;plot&#160;J&#160;for&#160;several&#160;learning&#160;rates&#160;on&#160;the&#160;same<br/>figure.&#160;In&#160;Octave,&#160;this&#160;can&#160;be&#160;done&#160;by&#160;performing&#160;gradient&#160;descent&#160;multi-<br/>ple&#160;times&#160;with&#160;a&#160;‘hold&#160;on’&#160;command&#160;between&#160;plots.&#160;Concretely,&#160;if&#160;you’ve<br/>tried&#160;three&#160;different&#160;values&#160;of&#160;alpha&#160;(you&#160;should&#160;probably&#160;try&#160;more&#160;values<br/>than&#160;this)&#160;and&#160;stored&#160;the&#160;costs&#160;in&#160;J1,&#160;J2&#160;and&#160;J3,&#160;you&#160;can&#160;use&#160;the&#160;following<br/>commands&#160;to&#160;plot&#160;them&#160;on&#160;the&#160;same&#160;figure:<br/>
plot(1:50,&#160;J1(1:50),&#160;‘b’);<br/>hold&#160;on;<br/>plot(1:50,&#160;J2(1:50),&#160;‘r’);<br/>plot(1:50,&#160;J3(1:50),&#160;‘k’);<br/>
The&#160;final&#160;arguments&#160;‘b’,&#160;‘r’,&#160;and&#160;‘k’&#160;specify&#160;different&#160;colors&#160;for&#160;the<br/>plots.<br/>
13<br/>
<hr/>
<a name=14></a>Notice&#160;the&#160;changes&#160;in&#160;the&#160;convergence&#160;curves&#160;as&#160;the&#160;learning&#160;rate&#160;changes.<br/>
With&#160;a&#160;small&#160;learning&#160;rate,&#160;you&#160;should&#160;find&#160;that&#160;gradient&#160;descent&#160;takes&#160;a&#160;very<br/>long&#160;time&#160;to&#160;converge&#160;to&#160;the&#160;optimal&#160;value.&#160;Conversely,&#160;with&#160;a&#160;large&#160;learning<br/>rate,&#160;gradient&#160;descent&#160;might&#160;not&#160;converge&#160;or&#160;might&#160;even&#160;diverge!<br/>
Using&#160;the&#160;best&#160;learning&#160;rate&#160;that&#160;you&#160;found,&#160;run&#160;the&#160;ex1&#160;multi.m&#160;script<br/>
to&#160;run&#160;gradient&#160;descent&#160;until&#160;convergence&#160;to&#160;find&#160;the&#160;final&#160;values&#160;of&#160;θ.&#160;Next,<br/>use&#160;this&#160;value&#160;of&#160;θ&#160;to&#160;predict&#160;the&#160;price&#160;of&#160;a&#160;house&#160;with&#160;1650&#160;square&#160;feet&#160;and<br/>3&#160;bedrooms.&#160;You&#160;will&#160;use&#160;value&#160;later&#160;to&#160;check&#160;your&#160;implementation&#160;of&#160;the<br/>normal&#160;equations.&#160;Don’t&#160;forget&#160;to&#160;normalize&#160;your&#160;features&#160;when&#160;you&#160;make<br/>this&#160;prediction!<br/>
You&#160;do&#160;not&#160;need&#160;to&#160;submit&#160;any&#160;solutions&#160;for&#160;these&#160;optional&#160;(ungraded)<br/>
exercises.<br/>
3.3<br/>
Normal&#160;Equations<br/>
In&#160;the&#160;lecture&#160;videos,&#160;you&#160;learned&#160;that&#160;the&#160;closed-form&#160;solution&#160;to&#160;linear<br/>regression&#160;is<br/>
−1<br/>
θ&#160;=&#160;XT&#160;X<br/>
XT&#160;y.<br/>
Using&#160;this&#160;formula&#160;does&#160;not&#160;require&#160;any&#160;feature&#160;scaling,&#160;and&#160;you&#160;will&#160;get<br/>
an&#160;exact&#160;solution&#160;in&#160;one&#160;calculation:&#160;there&#160;is&#160;no&#160;“loop&#160;until&#160;convergence”&#160;like<br/>in&#160;gradient&#160;descent.<br/>
Complete&#160;the&#160;code&#160;in&#160;normalEqn.m&#160;to&#160;use&#160;the&#160;formula&#160;above&#160;to&#160;calcu-<br/>
late&#160;θ.&#160;Remember&#160;that&#160;while&#160;you&#160;don’t&#160;need&#160;to&#160;scale&#160;your&#160;features,&#160;we&#160;still<br/>need&#160;to&#160;add&#160;a&#160;column&#160;of&#160;1’s&#160;to&#160;the&#160;X&#160;matrix&#160;to&#160;have&#160;an&#160;intercept&#160;term&#160;(θ0).<br/>The&#160;code&#160;in&#160;ex1.m&#160;will&#160;add&#160;the&#160;column&#160;of&#160;1’s&#160;to&#160;X&#160;for&#160;you.<br/>
You&#160;should&#160;now&#160;submit&#160;the&#160;normal&#160;equations&#160;function.<br/>
Optional&#160;(ungraded)&#160;exercise:&#160;Now,&#160;once&#160;you&#160;have&#160;found&#160;θ&#160;using&#160;this<br/>
method,&#160;use&#160;it&#160;to&#160;make&#160;a&#160;price&#160;prediction&#160;for&#160;a&#160;1650-square-foot&#160;house&#160;with<br/>3&#160;bedrooms.&#160;You&#160;should&#160;find&#160;that&#160;gives&#160;the&#160;same&#160;predicted&#160;price&#160;as&#160;the&#160;value<br/>you&#160;obtained&#160;using&#160;the&#160;model&#160;fit&#160;with&#160;gradient&#160;descent&#160;(in&#160;Section&#160;<a href="ex1s.html#13">3.2.1).</a><br/>
14<br/>
<hr/>
<a name=15></a>Submission&#160;and&#160;Grading<br/>
After&#160;completing&#160;various&#160;parts&#160;of&#160;the&#160;assignment,&#160;be&#160;sure&#160;to&#160;use&#160;the&#160;submit<br/>function&#160;system&#160;to&#160;submit&#160;your&#160;solutions&#160;to&#160;our&#160;servers.&#160;The&#160;following&#160;is&#160;a<br/>breakdown&#160;of&#160;how&#160;each&#160;part&#160;of&#160;this&#160;exercise&#160;is&#160;scored.<br/>
Part<br/>
Submitted&#160;File<br/>
Points<br/>
Warm&#160;up&#160;exercise<br/>
warmUpExercise.m<br/>
10&#160;points<br/>
Compute&#160;cost&#160;for&#160;one&#160;variable<br/>
computeCost.m<br/>
40&#160;points<br/>
Gradient&#160;descent&#160;for&#160;one&#160;variable<br/>
gradientDescent.m<br/>
50&#160;points<br/>
Total&#160;Points<br/>
100&#160;points<br/>
Extra&#160;Credit&#160;Exercises&#160;(optional)<br/>Feature&#160;normalization<br/>
featureNormalize.m<br/>
10&#160;points<br/>
Compute<br/>
cost<br/>
for<br/>
multiple<br/>
computeCostMulti.m<br/>
15&#160;points<br/>
variables<br/>Gradient&#160;descent&#160;for&#160;multiple<br/>
gradientDescentMulti.m<br/>
15&#160;points<br/>
variables<br/>Normal&#160;Equations<br/>
normalEqn.m<br/>
10&#160;points<br/>
You&#160;are&#160;allowed&#160;to&#160;submit&#160;your&#160;solutions&#160;multiple&#160;times,&#160;and&#160;we&#160;will&#160;take<br/>
only&#160;the&#160;highest&#160;score&#160;into&#160;consideration.&#160;To&#160;prevent&#160;rapid-fire&#160;guessing,&#160;the<br/>system&#160;enforces&#160;a&#160;minimum&#160;of&#160;5&#160;minutes&#160;between&#160;submissions.<br/>
15<br/>
<hr/>
<a name="outline"></a><h1>Document Outline</h1>
<ul>
<li><a href="ex1s.html#2">Simple octave function</a>
<ul>
<li><a href="ex1s.html#3">Submitting Solutions</a></li>
</ul>
</li>
<li><a href="ex1s.html#3">Linear regression with one variable</a>
<ul>
<li><a href="ex1s.html#4">Plotting the Data</a></li>
<li><a href="ex1s.html#4">Gradient Descent</a>
<ul>
<li><a href="ex1s.html#5">Update Equations</a></li>
<li><a href="ex1s.html#6">Implementation</a></li>
<li><a href="ex1s.html#6">Computing the cost J()</a></li>
<li><a href="ex1s.html#7">Gradient descent</a></li>
</ul>
</li>
<li><a href="ex1s.html#7">Debugging</a></li>
<li><a href="ex1s.html#8">Visualizing J()</a></li>
</ul>
</li>
<li><a href="ex1s.html#10">Linear regression with multiple variables</a>
<ul>
<li><a href="ex1s.html#10">Feature Normalization</a></li>
<li><a href="ex1s.html#11">Gradient Descent</a>
<ul>
<li><a href="ex1s.html#12">Optional (ungraded) exercise: Selecting learning rates</a></li>
</ul>
</li>
<li><a href="ex1s.html#14">Normal Equations</a></li>
</ul>
</li>
</ul>
<hr/>
</body>
</html>
